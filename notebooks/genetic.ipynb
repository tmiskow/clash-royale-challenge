{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Genetic search approach\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(420)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = np.load('../data/train_X.npy')\n",
    "train_y = np.load('../data/train_y.npy')\n",
    "valid_X = np.load('../data/valid_X.npy')\n",
    "valid_y = np.load('../data/valid_y.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 1500  # dataset size\n",
    "T = 20  # number of datasets per generation\n",
    "M = 0.1  # fraction of dataset samples dropped during mutation phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class DataSet(object):\n",
    "    X: np.array\n",
    "    y: np.array\n",
    "    ids: np.array\n",
    "    \n",
    "    @classmethod\n",
    "    def combine(cls, ds1, ds2):\n",
    "        assert(ds1.X.shape == ds2.X.shape)\n",
    "        intersection = np.intersect1d(ds1.ids, ds2.ids)\n",
    "        other = np.random.choice(\n",
    "            np.setxor1d(ds1.ids, ds2.ids), len(ds1.ids)-len(intersection)\n",
    "        )\n",
    "        new_ids = np.concatenate([intersection, other])\n",
    "        take_from_left = np.isin(ds1.ids, new_ids)\n",
    "        take_from_right = np.isin(ds2.ids, new_ids)\n",
    "        # new ids need to be re-ordered to match the order \n",
    "        # in which X and y are selected for the new DataSet:\n",
    "        new_ids = np.select(\n",
    "            [take_from_left, take_from_right],\n",
    "            [ds1.ids, ds2.ids]\n",
    "        )\n",
    "        new_X = np.select(\n",
    "            [\n",
    "                np.repeat(take_from_left, ds1.X.shape[1]).reshape(ds1.X.shape),\n",
    "                np.repeat(take_from_right, ds2.X.shape[1]).reshape(ds2.X.shape)\n",
    "            ],\n",
    "            [ds1.X, ds2.X]\n",
    "        )\n",
    "        new_y = np.select(\n",
    "            [take_from_left, take_from_right],\n",
    "            [ds1.y, ds2.y]\n",
    "        )\n",
    "        return cls(new_X, new_y, new_ids)\n",
    "        \n",
    "\n",
    "def sample_datasets(\n",
    "        n_samples: int=N, \n",
    "        n_datasets: int=T, \n",
    "        n_validation_samples: int=10*N, \n",
    "        source_X: np.array=train_X, \n",
    "        source_y: np.array=train_y\n",
    ") -> Tuple[List[DataSet], DataSet]:\n",
    "    ids = [\n",
    "        np.random.choice(\n",
    "            len(source_X), \n",
    "            n_samples, \n",
    "            replace=False\n",
    "        ) for i in range(n_datasets)\n",
    "    ]\n",
    "    rest_ids = np.array(list(set(range(len(source_X))) - set(list(np.concatenate(ids)))))\n",
    "    validation_ids = np.random.choice(rest_ids, n_validation_samples)\n",
    "    return [DataSet(source_X[i], source_y[i], i) for i in ids], DataSet(source_X[validation_ids], source_y[validation_ids], validation_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import ParameterSampler\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "params_dict = {\n",
    "    'kernel': ['rbf'],\n",
    "    'gamma': [1 / i for i in range(80, 130, 10)],\n",
    "    'C': [0.9, 1.0, 1.1],\n",
    "    'epsilon': [1e-3, 3e-3, 1e-2, 3e-2, 1e-1, 3e-1],\n",
    "    'shrinking': [True]\n",
    "}\n",
    "\n",
    "def train_svr(train_set: DataSet, param_validation_set: DataSet, params_dict: dict=params_dict, n_iter: int=20):\n",
    "    ps = ParameterSampler(n_iter=n_iter, param_distributions=params_dict)\n",
    "    scores = np.zeros(n_iter)\n",
    "    models = list()\n",
    "    for idx, params in enumerate(ps):\n",
    "        svr = SVR(**params)\n",
    "        svr.fit(train_set.X, train_set.y)\n",
    "        preds = svr.predict(param_validation_set.X)\n",
    "        scores[idx] = r2_score(param_validation_set.y, preds)\n",
    "        models.append(svr)\n",
    "    return models[np.argmax(scores)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from itertools import combinations\n",
    "\n",
    "class VerboseMixin(object):\n",
    "    def _progress(self, iterator, total):\n",
    "        if self.verbose:\n",
    "            return tqdm(iterator, total=total, desc=self.__class__.__name__)\n",
    "        else:\n",
    "            return iterator\n",
    "\n",
    "    def _log(self, message):\n",
    "        if self.verbose:\n",
    "            print(f\"[{self.__class__.__name__}] {message}\")\n",
    "\n",
    "class Evolution(VerboseMixin):\n",
    "    def __init__(\n",
    "            self, \n",
    "            T=T, \n",
    "            train_X=train_X, \n",
    "            train_y=train_y, \n",
    "            valid_X=valid_X, \n",
    "            valid_y=valid_y, \n",
    "            n_generations: int=100,\n",
    "            verbose: bool=False\n",
    "    ):\n",
    "        self.training_sets, self.param_validation_set = sample_datasets(T)\n",
    "        self.entire_dataset = DataSet(train_X, train_y, np.arange(len(train_X)))\n",
    "        self.model_validation_dataset = DataSet(valid_X, valid_y, None)\n",
    "        self.n_generations = n_generations\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def _train(self):\n",
    "        sample_scores = np.zeros(\n",
    "            (len(self.training_sets), len(self.entire_dataset.X))\n",
    "        )  # sample_scores[model_id][sample] = model's uncertainty of this sample\n",
    "        model_validation_scores = np.zeros(len(self.training_sets))\n",
    "        model_params = list()\n",
    "        for model_id, ds in self._progress(\n",
    "                enumerate(self.training_sets), total=len(self.training_sets)\n",
    "        ):\n",
    "            model = train_svr(ds, self.param_validation_set)\n",
    "            preds = model.predict(self.entire_dataset.X)\n",
    "            sample_scores[model_id] = np.abs(self.entire_dataset.y - preds)\n",
    "            preds = model.predict(self.model_validation_dataset.X)\n",
    "            model_validation_scores[model_id] = r2_score(self.model_validation_dataset.y, preds)\n",
    "            model_params.append(model.get_params())\n",
    "        return np.mean(sample_scores * model_validation_scores.reshape((-1,1)), axis=0), model_params, model_validation_scores\n",
    "    \n",
    "    def _select_sets(self, model_validation_scores: np.array):\n",
    "        normalized_scores = model_validation_scores / np.sum(model_validation_scores)\n",
    "        sorted_order = np.argsort(-normalized_scores)  # sort by DESCENDING SCORE\n",
    "        self.training_sets = list(np.array(self.training_sets)[sorted_order])\n",
    "        cum_scores = np.cumsum(normalized_scores[sorted_order])\n",
    "        fitness_threshold = 0.\n",
    "        while fitness_threshold < cum_scores[1]:\n",
    "            fitness_threshold = np.random.random()\n",
    "        fit_datasets = [\n",
    "            ds for ds, is_fit \n",
    "            in zip(self.training_sets, cum_scores >= fitness_threshold) \n",
    "            if is_fit\n",
    "        ]\n",
    "        return fit_datasets  # sorted by model score, descending\n",
    "    \n",
    "    def _crossover(self, fit_datasets: List[np.array]):\n",
    "        new_datasets = []\n",
    "        while len(new_datasets) < len(self.training_sets):\n",
    "            # if there is a really small number of fit_datasets, \n",
    "            # we want to resample T new datasets from what we have\n",
    "            for ds1, ds2 in combinations(fit_datasets, 2):\n",
    "                new_datasets.append(DataSet.combine(ds1, ds2))\n",
    "                if len(new_datasets) >= len(self.training_sets):\n",
    "                    break\n",
    "        self.training_sets = new_datasets\n",
    "        \n",
    "    def _mutate(self):\n",
    "        raise NotImplemented()\n",
    "    \n",
    "    def __iter__(self):\n",
    "        self.generation = 0\n",
    "        return self\n",
    "    \n",
    "    def __next__(self):\n",
    "        if self.generation >= self.n_generations:\n",
    "            raise StopIteration()\n",
    "        weighted_sample_scores, model_params, model_validation_scores = self._train()\n",
    "        fit_datasets = self._select_sets(model_validation_scores)\n",
    "        self._crossover(fit_datasets)\n",
    "#         self._mutate()  # TODO\n",
    "        self.generation += 1\n",
    "        return model_validation_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evolution: 100%|████████████████████████████████████████████████████████████████████| 20/20 [00:21<00:00,  1.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.08068123 -0.06492513 -0.12409585 -0.28681169 -0.30541897 -0.05833036\n",
      " -0.11678386 -0.05277021 -0.29675168 -0.29117177 -0.09809191 -0.2452606\n",
      " -0.01433659 -0.12487273 -0.26254136 -0.32474264 -0.17549072 -0.03588393\n",
      " -0.08146849 -0.01013139]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evolution: 100%|████████████████████████████████████████████████████████████████████| 20/20 [00:30<00:00,  1.60s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ -1.55287063  -8.8277393  -13.11085259  -3.3199114   -1.16000664\n",
      "  -2.32638548  -0.62343323  -1.05061934  -0.49077914 -15.81230812\n",
      "  -6.27466876  -1.4552272   -8.00177109  -0.41777791  -1.04832412\n",
      "  -1.64854863 -15.88610032  -5.98064997  -4.58690519  -3.68839728]\n"
     ]
    }
   ],
   "source": [
    "ev = Evolution(n_generations=2, verbose=True)\n",
    "for scores in ev:\n",
    "    print(scores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
