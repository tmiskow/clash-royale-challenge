{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ../src/genetic.py\n",
    "\"\"\"\n",
    "Fast, parallelizable genetic algorithm implementation with some shortcuts.\n",
    "\n",
    "Numpy array naming convention:\n",
    "- *_ids = array of number corresponding to rows in the dataset\n",
    "- *_index = boolean array allowing for fast selection from the dataset\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.random.seed(420)\n",
    "\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import ParameterSampler\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "from tqdm import trange\n",
    "from numba import jit\n",
    "import click\n",
    "\n",
    "import multiprocessing as mp\n",
    "import pickle\n",
    "from itertools import repeat\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import NamedTuple, List, Tuple\n",
    "\n",
    "\n",
    "class DataSet(NamedTuple):\n",
    "    X: np.array  # (n_samples, n_features)\n",
    "    y: np.array  # 1d\n",
    "    ids: np.array  # 1d\n",
    "\n",
    "\n",
    "class GenerationParams(NamedTuple):\n",
    "    n_models: int\n",
    "    train_ids: np.array  # (n_models, n_train_samples)\n",
    "    n_fits: int  # per each model during hyperparameter optimization\n",
    "    train_data: DataSet\n",
    "    n_train_samples: int\n",
    "    train_probs: np.array\n",
    "    valid_data: DataSet\n",
    "    valid_index: np.array\n",
    "    mutation_prob: float\n",
    "    score_mode: str\n",
    "\n",
    "class FitParams(NamedTuple):\n",
    "    train_index: np.array # 1d train_size\n",
    "    n_fits: int\n",
    "    train_data: DataSet\n",
    "    valid_data: DataSet\n",
    "    valid_index: np.array\n",
    "\n",
    "\n",
    "class FitResult(NamedTuple):\n",
    "    train_index: np.array\n",
    "    model_pred: np.array\n",
    "    model_score: float\n",
    "    model_params: dict\n",
    "\n",
    "\n",
    "class MutationParams(NamedTuple):\n",
    "    pair: np.array\n",
    "    train_probs: np.array\n",
    "    mutation_prob: float\n",
    "\n",
    "\n",
    "class GenerationResult(NamedTuple):\n",
    "    train_probs: np.array\n",
    "    # train_index: np.array  # (n_samples), True for samples that were used in any of the models\n",
    "    model_scores: np.array\n",
    "    model_params: List[dict]\n",
    "    model_samples: np.array  # (n_models, n_train_samples) - IDs, NOT INDEX\n",
    "\n",
    "\n",
    "class EvolutionParams(NamedTuple):\n",
    "    n_models: int  # = datasets per generation\n",
    "    n_fits: int  # per each model during hyperparameter optimization\n",
    "    n_generations: int\n",
    "    n_train_samples: int\n",
    "    n_valid_samples: int\n",
    "    train_ids: np.array\n",
    "    mutation_prob: float  # between 0 and 1\n",
    "    score_mode: str\n",
    "\n",
    "\n",
    "# defaults for random hyperparameter search\n",
    "params_dict = {\n",
    "    'kernel': ['rbf'],\n",
    "    'gamma': [1 / i for i in range(60, 130, 20)],\n",
    "    'C': [0.9, 1.0, 1.1],\n",
    "    'epsilon': [1e-2, 3e-2, 5e-2],\n",
    "    'shrinking': [True]\n",
    "}\n",
    "\n",
    "\n",
    "def sample(n_samples: int, ids: np.array, weights: np.array=None) -> np.array:\n",
    "    selected_ids = np.random.choice(ids, n_samples, replace=False, p=weights)\n",
    "    selected_index = np.isin(ids, selected_ids, assume_unique=True)\n",
    "    return selected_index  # same shape as ids, for easier selection\n",
    "\n",
    "\n",
    "def fit_svr(\n",
    "        X_train: np.array,\n",
    "        y_train: np.array,\n",
    "        X_valid: np.array,\n",
    "        y_valid: np.array,\n",
    "        params_dict: dict=params_dict,\n",
    "        n_iter: int=25\n",
    "):\n",
    "    ps = ParameterSampler(n_iter=n_iter, param_distributions=params_dict)\n",
    "    scores = np.zeros(n_iter)\n",
    "    models = list(repeat(None, n_iter))\n",
    "    for idx, params in enumerate(ps):\n",
    "        svr = SVR(**params)\n",
    "        svr.fit(X_train, y_train)\n",
    "        scores[idx] = r2_score(y_valid, svr.predict(X_valid))\n",
    "        models[idx] = svr\n",
    "    return models[np.argmax(scores)]\n",
    "\n",
    "\n",
    "def fit_thread(params: GenerationParams) -> FitResult:\n",
    "    model = fit_svr(\n",
    "        params.train_data.X[params.train_index],\n",
    "        params.train_data.y[params.train_index],\n",
    "        params.valid_data.X[params.valid_index],\n",
    "        params.valid_data.y[params.valid_index],\n",
    "        n_iter=params.n_fits\n",
    "    )\n",
    "    model_pred = model.predict(params.train_data.X)\n",
    "    model_score = r2_score(\n",
    "        params.valid_data.y[params.valid_index],\n",
    "        model.predict(params.valid_data.X[params.valid_index])\n",
    "    )\n",
    "    return FitResult(params.train_index, model_pred, model_score, model.get_params())\n",
    "\n",
    "\n",
    "def crossover_thread(params: MutationParams):\n",
    "    intersection = np.intersect1d(params.pair[0], params.pair[1])\n",
    "    rest = np.setxor1d(params.pair[0], params.pair[1])\n",
    "    if len(rest) == 0:\n",
    "        new_ids = intersection\n",
    "    else:\n",
    "        other = np.random.choice(\n",
    "            rest, len(params.pair[0]) - len(intersection), replace=False,\n",
    "        )\n",
    "        new_ids = np.concatenate([intersection, other])\n",
    "    assert len(new_ids) == len(params.pair[0])\n",
    "    # Mutation\n",
    "    np.random.shuffle(new_ids)\n",
    "    survival_boundary = round(len(new_ids) * params.mutation_prob)\n",
    "    chosen = new_ids[:-survival_boundary]\n",
    "    params.train_probs[chosen.astype(np.int)] = 0\n",
    "    assert params.train_probs[chosen.astype(np.int)].sum() == 0\n",
    "    adjusted_params = params.train_probs / params.train_probs.sum()\n",
    "    assert adjusted_params[chosen.astype(np.int)].sum() == 0\n",
    "    supplied = np.random.choice(\n",
    "        np.arange(len(params.train_probs)),\n",
    "        size=survival_boundary,\n",
    "        replace=False,\n",
    "        p=adjusted_params\n",
    "    )\n",
    "    result = np.concatenate([chosen, supplied])\n",
    "    assert len(result) == params.pair.shape[1]\n",
    "    return result.astype(np.int)\n",
    "\n",
    "\n",
    "def select_best(model_scores):\n",
    "    normalized_scores = model_scores / np.sum(model_scores)\n",
    "    sorted_order = np.argsort(-normalized_scores)  # sort by DESCENDING SCORE\n",
    "    cum_scores = np.cumsum(normalized_scores[sorted_order])\n",
    "    fitness_threshold = (np.random.random() / cum_scores[1]) + cum_scores[1]\n",
    "    fit_scores = cum_scores < fitness_threshold\n",
    "    return sorted_order[fit_scores]\n",
    "\n",
    "\n",
    "def calculate_probs(models_pred: np.array, mode: str, y: np.array=None):\n",
    "    if mode == \"variance\":\n",
    "        train_var = models_pred.var(axis=0)\n",
    "        assert len(train_var) == models_pred.shape[1]\n",
    "        scores = train_var\n",
    "    elif mode == \"loss\":\n",
    "        if y is None:\n",
    "            raise AttributeError(f\"y can't be None when using 'loss' mode\")\n",
    "        loss = np.subtract(models_pred, y.reshape(1, -1)) ** 2\n",
    "        scores = loss.mean(axis=0)\n",
    "    else:\n",
    "        raise AttributeError(f\"No such method: {mode}\")\n",
    "\n",
    "    exploded_scores = np.exp(scores)\n",
    "\n",
    "    # zeroout prob of half of the samples that were predicted correctly\n",
    "    sorted_order_ids = np.argsort(exploded_scores)  # sort by ASCENDING SCORE\n",
    "    cum_scores = np.cumsum(exploded_scores[sorted_order_ids])\n",
    "    unfit_index = (cum_scores / cum_scores[-1]) < 0.5  # eliminate most accurately predicted samples\n",
    "    exploded_scores[sorted_order_ids[unfit_index]] = 0\n",
    "\n",
    "    normalized_scores = exploded_scores / exploded_scores.sum()\n",
    "    return normalized_scores\n",
    "\n",
    "\n",
    "def run_generation(params: GenerationParams, pool: mp.Pool) -> (GenerationParams, GenerationResult):\n",
    "    models_pred = np.zeros((params.n_models, len(params.train_probs)))\n",
    "    model_params = list(repeat({}, params.n_models))\n",
    "    model_scores = np.zeros(params.n_models)\n",
    "    model_samples = np.zeros((params.n_models, params.n_train_samples))\n",
    "\n",
    "    fit_params = [FitParams(\n",
    "        train_index=params.train_ids[i],\n",
    "        n_fits=params.n_fits,\n",
    "        train_data=params.train_data,\n",
    "        valid_data=params.valid_data,\n",
    "        valid_index=params.valid_index,\n",
    "    ) for i in range(len(params.train_ids))]\n",
    "\n",
    "    results = pool.map(fit_thread, fit_params)\n",
    "    for idx, fit_result in enumerate(results):\n",
    "        models_pred[idx] = fit_result.model_pred\n",
    "        model_params[idx] = fit_result.model_params\n",
    "        model_scores[idx] = fit_result.model_score\n",
    "        model_samples[idx] = params.train_data.ids[fit_result.train_index].astype(np.uint)\n",
    "\n",
    "    chosen_samples = model_samples[select_best(model_scores)]\n",
    "    comb = np.random.randint(low=0, high=len(chosen_samples), size=(params.n_models, 2))\n",
    "    pairs = [chosen_samples[c] for c in comb]\n",
    "    train_probs = calculate_probs(models_pred, mode=params.score_mode, y=params.train_data.y)\n",
    "    crossover_param_list = [MutationParams(pair=p, train_probs=train_probs, mutation_prob=params.mutation_prob) for p in pairs]\n",
    "    new_ids = pool.map(crossover_thread, crossover_param_list)\n",
    "\n",
    "    new_params = params._replace(train_ids=np.asarray(new_ids), train_probs=train_probs)\n",
    "    return new_params, GenerationResult(train_probs, model_scores, model_params, model_samples)\n",
    "\n",
    "\n",
    "def run_evolution(train_data: DataSet, valid_data: DataSet, pool: mp.Pool, params: EvolutionParams):\n",
    "    valid_index = sample(params.n_valid_samples, valid_data.ids)\n",
    "    train_probs = np.ones(len(train_data.ids)) / len(train_data.ids)\n",
    "    results = []\n",
    "    if params.train_ids is None:\n",
    "        train_ids = np.random.choice(\n",
    "            train_data.ids,\n",
    "            size=(params.n_models, params.n_train_samples),\n",
    "            replace=False,\n",
    "        )\n",
    "    else:\n",
    "        train_ids = params.train_ids\n",
    "\n",
    "    next_gen_params = GenerationParams(\n",
    "        n_models=params.n_models,\n",
    "        n_fits=params.n_fits,\n",
    "        train_data=train_data,\n",
    "        n_train_samples=params.n_train_samples,\n",
    "        train_probs=train_probs,\n",
    "        valid_data=valid_data,\n",
    "        valid_index=valid_index,\n",
    "        mutation_prob=params.mutation_prob,\n",
    "        train_ids=train_ids,\n",
    "        score_mode=params.score_mode,\n",
    "    )\n",
    "\n",
    "    with trange(params.n_generations) as t:\n",
    "        for generation_idx in t:\n",
    "            try:\n",
    "                t.set_description(f\"Generation {generation_idx+1}\")\n",
    "                next_gen_params, gen_results = run_generation(next_gen_params, pool)\n",
    "                results.append(gen_results)\n",
    "                t.set_postfix(\n",
    "                    mean_score=sum(gen_results.model_scores)/len(gen_results.model_scores),\n",
    "                    max_score=max(gen_results.model_scores)\n",
    "                )\n",
    "            except KeyboardInterrupt:\n",
    "                print(\"KeyboardInterrupt: stopping evolution...\")\n",
    "                break\n",
    "    return results\n",
    "\n",
    "\n",
    "@click.command()\n",
    "@click.option(\"-n\", \"--n-threads\", default=4)\n",
    "@click.option(\"-i\", \"--input-dir\", type=str, default='../data')\n",
    "@click.option(\"-o\", \"--output-path\", type=str,\n",
    "              default=datetime.now().strftime('../data/genetic-%d-%m-%y-%H-%M-%S.pkl'))\n",
    "def main(n_threads, input_dir, output_path):\n",
    "    input_dir = Path(input_dir).resolve()\n",
    "    output_path = Path(output_path).resolve()\n",
    "    train_X = np.load(input_dir / 'train_X.npy')\n",
    "    train_y = np.load(input_dir / 'train_y.npy')\n",
    "    valid_X = np.load(input_dir / 'valid_X.npy')\n",
    "    valid_y = np.load(input_dir / 'valid_y.npy')\n",
    "    train_data = DataSet(train_X, train_y, np.arange(len(train_X)))\n",
    "    valid_data = DataSet(valid_X, valid_y, np.arange(len(valid_X)) * (-1))\n",
    "    params = EvolutionParams(\n",
    "        n_models = 32,\n",
    "        n_fits = 32,\n",
    "        n_generations = 128,\n",
    "        n_train_samples = 1500,\n",
    "        n_valid_samples = 6000,\n",
    "        train_ids = None,\n",
    "        mutation_prob = 0.04,\n",
    "        score_mode = \"variance\",\n",
    "    )\n",
    "    with mp.Pool(n_threads) as pool:\n",
    "        results = run_evolution(train_data, valid_data, pool, params)\n",
    "        print(f\"Saving results to {output_path}...\")\n",
    "        pickle.dump(results, open(output_path, 'wb'))\n",
    "        print(\"Done\")\n",
    "\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = Path('../data/')\n",
    "train_X = np.load(input_dir / 'train_X.npy')\n",
    "train_y = np.load(input_dir / 'train_y.npy')\n",
    "valid_X = np.load(input_dir / 'valid_X.npy')\n",
    "valid_y = np.load(input_dir / 'valid_y.npy')\n",
    "train_data = DataSet(train_X, train_y, np.arange(len(train_X)))\n",
    "valid_data = DataSet(valid_X, valid_y, np.arange(len(valid_X)) * (-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fitting the model on validation data\n",
    "model = fit_svr(valid_X, valid_y, train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(train_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.02438919832327191"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# most of the training data is not well explained by validation features\n",
    "r2_score(train_y, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples matching selector: 1852\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.998830512688503"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# but some of the training samples are\n",
    "mse = (y_pred-train_y)**2\n",
    "sample_selector = mse < 1e-6\n",
    "print(f\"Samples matching selector: {sum(sample_selector)}\")\n",
    "r2_score(train_y[sample_selector], y_pred[sample_selector])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.17417106592376708"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# so we fit the model on these samples\n",
    "normal_model = fit_svr(train_X[sample_selector], train_y[sample_selector], valid_X, valid_y, n_iter=10)\n",
    "r2_score(valid_y, normal_model.predict(valid_X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo: this can potentially serve as a good starting point for genetic algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "sampling_prob = mse / sum(mse)\n",
    "plt.hist(sampling_prob)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DescribeResult(nobs=100000, minmax=(2.1110198703677774e-17, 0.0006615211651541762), mean=9.999999999999857e-06, variance=6.262755722361359e-10, skewness=10.671713917530614, kurtosis=186.82272911289297)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.stats import describe\n",
    "describe(sampling_prob)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
